{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Text Normalization\n",
    "\n",
    "Atleast three steps are involved to normalize the text:\n",
    "\n",
    "* ***Segmenting/tokenizing words from running text***\n",
    "* ***Normalizing word formats***\n",
    "* * Convert characters to lowercase\n",
    "* * Expand Abbreviations\n",
    "* * Remove stopwords\n",
    "* * Lemmatizers\n",
    "* * Stemmatizers\n",
    "* ***Segmenting sentences in running text***\n",
    "\n",
    "* * ***Spacy do not implement stemmatizers***\n",
    "* * * Reference: https://github.com/explosion/spaCy/issues/327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.en import English\n",
    "from __future__ import unicode_literals \n",
    "nlp = spacy.load('en')  # load english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class text_normalization:\n",
    "    def __init__(self,parser):\n",
    "        self.punctuations = list(string.punctuation)\n",
    "        self.parser = parser\n",
    "        \n",
    "    '''\n",
    "    This function takes text as input and returns\n",
    "    list of tokenized words\n",
    "    '''\n",
    "    def word_tokenizer(self,text):\n",
    "        list_of_tokenized_words = []\n",
    "        lowercase_words = []\n",
    "        # parse the data\n",
    "        parsedData = self.parser(text)\n",
    "        for token in parsedData:\n",
    "            # tokenize the word \n",
    "            # each token contains properties\n",
    "            # property with underscore(token_orth_) returns string\n",
    "            # property without underscore(token_orth) returns an index (int) into spaCy's vocabulary\n",
    "            lower_case = self.convert_characters_lower_case(token)\n",
    "            token =  token.orth_\n",
    "\n",
    "            if token in self.punctuations:\n",
    "                continue\n",
    "            else:\n",
    "                lowercase_words.append(lower_case)\n",
    "                list_of_tokenized_words.append(token)\n",
    "        print \"Original text: \\n \", text\n",
    "        print \"=================================================================================\"\n",
    "        print \"Tokenized sentences are : \\n \", self.sent_tokenizer(text)\n",
    "        print \"=================================================================================\"\n",
    "        print \"List of tokenized words without punctutations: \\n \" + str(list_of_tokenized_words)  \n",
    "        print \"=================================================================================\"\n",
    "        print \"Length of words: \", len(list_of_tokenized_words)\n",
    "        print \"=================================================================================\"\n",
    "        print \"Lowercase words: \\n \" + str(lowercase_words)\n",
    "        print \"=================================================================================\"\n",
    "        return list_of_tokenized_words\n",
    "        \n",
    "        '''\n",
    "        Convert characters of each token to lower case\n",
    "        '''\n",
    "    def convert_characters_lower_case(self,token):\n",
    "        return token.lower_\n",
    "        \n",
    "        '''\n",
    "        This function takes sentences as input and\n",
    "        returns list of tokenized sentences\n",
    "        '''\n",
    "        \n",
    "    def sent_tokenizer(self,sentences):\n",
    "        document =  nlp(sentences)\n",
    "        list_of_tokenized_sentence = []\n",
    "        for sentence in document.sents:\n",
    "            list_of_tokenized_sentence.append(sentence)\n",
    "        \n",
    "        return list_of_tokenized_sentence\n",
    "        \n",
    "        '''\n",
    "        This function returns list of lemma \n",
    "        in the text\n",
    "        '''\n",
    "    def lemma(self,text):\n",
    "        lemma_list = []\n",
    "        parsedData = self.parser(text)\n",
    "        for word in parsedData:\n",
    "            word = word.lemma_\n",
    "\n",
    "            if word in self.punctuations:\n",
    "                continue\n",
    "            else:\n",
    "                lemma_list.append(word)\n",
    "                \n",
    "        print \"Lemma of text is: \\n \",lemma_list\n",
    "        return lemma_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "  San Francisco is part of the USA, where many peoples are living. Bonn is the part of Germany\n",
      "=================================================================================\n",
      "Tokenized sentences are : \n",
      "  [San Francisco is part of the USA, where many peoples are living., Bonn is the part of Germany]\n",
      "=================================================================================\n",
      "List of tokenized words without punctutations: \n",
      " [u'San', u'Francisco', u'is', u'part', u'of', u'the', u'USA', u'where', u'many', u'peoples', u'are', u'living', u'Bonn', u'is', u'the', u'part', u'of', u'Germany']\n",
      "=================================================================================\n",
      "Length of words:  18\n",
      "=================================================================================\n",
      "Lowercase words: \n",
      " [u'san', u'francisco', u'is', u'part', u'of', u'the', u'usa', u'where', u'many', u'peoples', u'are', u'living', u'bonn', u'is', u'the', u'part', u'of', u'germany']\n",
      "=================================================================================\n",
      "Lemma of text is: \n",
      "  [u'san', u'francisco', u'be', u'part', u'of', u'the', u'usa', u'where', u'many', u'people', u'be', u'live', u'bonn', u'be', u'the', u'part', u'of', u'germany']\n"
     ]
    }
   ],
   "source": [
    "text_normalizer = text_normalization(parser)\n",
    "text =\"San Francisco is part of the USA, where many peoples are living. Bonn is the part of Germany\"\n",
    "\n",
    "word_tokenizer = text_normalizer.word_tokenizer(text)\n",
    "lemma = text_normalizer.lemma(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
