{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy vs NLTK\n",
    "\n",
    "## Spacy \n",
    " \n",
    "over 400 times faster\n",
    "\n",
    "State-of-the art accuracy\n",
    "\n",
    "Tokenizer maintains alignment\n",
    "\n",
    "Powerful, concise API\n",
    "\n",
    "Integrated word vectors\n",
    "\n",
    "\n",
    "## NLTK\n",
    "\n",
    "Slow \n",
    "\n",
    "Low accuracy\n",
    "\n",
    "Tokens do not align to original string\n",
    "\n",
    "Models return list of strings\n",
    "\n",
    "No word vector support\n",
    "\n",
    "## References:\n",
    "\n",
    "https://spacy.io/docs/api/\n",
    "\n",
    "https://www.quora.com/What-are-the-advantages-of-Spacy-vs-NLTK\n",
    "\n",
    "https://spacy.io/docs/api/language-models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "import string\n",
    "from spacy.en import English\n",
    "# this is included because spacy is written in python 2 and python 3 but sometimes\n",
    "# for strings it uses python 3 therefore, we need to include this line \n",
    "# to avoid writing \"u\" for unicode before writing text\n",
    " \n",
    "nlp = spacy.load('en')  # load english language module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenize:\n",
    "    def __init__(self,parser):\n",
    "        self.parser = parser\n",
    "        self.punctuations = list(string.punctuation)\n",
    "        \n",
    "    def word_tokenizer(self,text):\n",
    "        list_of_tokenized_words = []\n",
    "        lowercase_words = []\n",
    "        # parse the data\n",
    "        parsedData = self.parser(text)\n",
    "        for token in parsedData:\n",
    "            # tokenize the word \n",
    "            # each token contains properties\n",
    "            # property with underscore(token_orth_) returns string\n",
    "            # property without underscore(token_orth) returns an index (int) into spaCy's vocabulary\n",
    "            lower_case = self.convert_characters_lower_case(token)\n",
    "            token =  token.orth_\n",
    "            # remove punctuation marks\n",
    "            if token in self.punctuations:\n",
    "                continue\n",
    "            else:\n",
    "                lowercase_words.append(lower_case)\n",
    "                list_of_tokenized_words.append(token)\n",
    "        print \"Original text: \\n \", text\n",
    "        print \"=========================================================\"\n",
    "        print \"List of tokenized words without punctutations: \\n \" + str(list_of_tokenized_words)  \n",
    "        print \"=========================================================\"\n",
    "        print \"Length of words: \", len(list_of_tokenized_words)\n",
    "        print \"=========================================================\"\n",
    "        print \"Lowercase words: \\n \" + str(lowercase_words)\n",
    "        \n",
    "    \n",
    "    def convert_characters_lower_case(self,token):\n",
    "        return token.lower_\n",
    "        \n",
    "    def sent_tokenizer(self,sentences):\n",
    "        document =  nlp(sentences)\n",
    "        list_of_tokenized_sentence = []\n",
    "        for sentence in document.sents:\n",
    "            list_of_tokenized_sentence.append(sentence)\n",
    "        \n",
    "        print \"Original text: \\n \", sentences\n",
    "        print \"==================================================================================\"\n",
    "        print \"List of tokenized sentences are: \\n \", list_of_tokenized_sentence\n",
    "        print \"==================================================================================\"\n",
    "        print \"Number of sentences are: \\n\", len(list_of_tokenized_sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = Tokenize(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "## Issues \n",
    "\n",
    "San Francisco is one word, but tokenizer has considered it two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "  San Francisco is part of the USA, where many peoples are living.\n",
      "=========================================================\n",
      "List of tokenized words without punctutations: \n",
      " [u'San', u'Francisco', u'is', u'part', u'of', u'the', u'USA', u'where', u'many', u'peoples', u'are', u'living']\n",
      "=========================================================\n",
      "Length of words:  12\n",
      "=========================================================\n",
      "Lowercase words: \n",
      " [u'san', u'francisco', u'is', u'part', u'of', u'the', u'usa', u'where', u'many', u'peoples', u'are', u'living']\n"
     ]
    }
   ],
   "source": [
    "text =\"San Francisco is part of the USA, where many peoples are living.\"\n",
    "\n",
    "tokenize.word_tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenizer\n",
    "\n",
    "Sentences are tokenized based on periods(.), exclamation marks(!) and question marks(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "  This is first sentence. This is second sentence! Let's try to tokenize the sentences. how are you? I am doing good\n",
      "==================================================================================\n",
      "List of tokenized sentences are: \n",
      "  [This is first sentence., This is second sentence!, Let's try to tokenize the sentences., how are you?, I am doing good]\n",
      "==================================================================================\n",
      "Number of sentences are: \n",
      "5\n"
     ]
    }
   ],
   "source": [
    "multipleSentence = \"This is first sentence. This is second sentence! Let's try to tokenize the sentences. how are you? I am doing good\"\n",
    "tokenize.sent_tokenizer(multipleSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
