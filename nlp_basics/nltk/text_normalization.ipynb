{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Text Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for stemming, Porter Stemmer algorithm is used .\n",
    "\n",
    "Other available algorithm for stemming in NLTK is Snowball stemmer, Lancaster stemmers\n",
    "\n",
    "The WordNet lemmatizer removes affixes only if the resulting word is in its dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class text_normalization:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.punctuations = list(string.punctuation)\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemma = WordNetLemmatizer()\n",
    "    '''\n",
    "    This function takes text as input \n",
    "    and returns list of tokenized words(punctuations not included) \n",
    "    '''    \n",
    "    def word_tokenizer(self,text):\n",
    "        self.word_tokenize_list = word_tokenize(text)\n",
    "\n",
    "        for word in self.word_tokenize_list:\n",
    "            if word in self.punctuations:\n",
    "                self.word_tokenize_list.remove(word)        \n",
    "        return self.word_tokenize_list\n",
    "    '''\n",
    "    This function takes text as input \n",
    "    and returns list of tokenized sentences \n",
    "    '''    \n",
    "    def sentence_tokenizer(self,sentence):\n",
    "        self.sentence_tokenize_list = sent_tokenize(sentence)\n",
    "        return self.sentence_tokenize_list\n",
    "    \n",
    "    '''\n",
    "    This function takes tokenized sentence as input \n",
    "    and returns lemma of tokenized sentence \n",
    "    \n",
    "    1. First sentences from text are tokenized\n",
    "    2. Then, words from sentences are tokenized\n",
    "    3. Lemmatizer of words is determined\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def lemmatizer(self, sentences):\n",
    "        token_sent = self.sentence_tokenizer(sentence)\n",
    "        token_word = [self.word_tokenizer(token_sent[i]) for i in range(len(token_sent))]\n",
    "\n",
    "        for noOfSent in range(len(token_word)):\n",
    "            stem = [(self.lemma.lemmatize((token_word[noOfSent][words]))) for words in range(len(token_word[noOfSent]))]\n",
    "            stem = \" \".join(stem)            \n",
    "            print \"tokenized sentence is: \" + str(token_sent[noOfSent])\n",
    "            print \"lemma of this sentence is: \" +  str(stem)\n",
    "            print \"tokenized words in lemmatizer sentence are: \" + str(self.word_tokenizer(stem))\n",
    "            print \"=========================================================================\"\n",
    "    '''\n",
    "    This function takes tokenized sentence as input \n",
    "    and returns stemmer of tokenized sentence \n",
    "    \n",
    "    1. First sentences from text are tokenized\n",
    "    2. Then, words from sentences are tokenized\n",
    "    3. Porter stemmer algorithm is applied on tokenized words\n",
    "    \n",
    "    '''       \n",
    "    def porter_stemmer(self,sentence):\n",
    "        token_sent = self.sentence_tokenizer(sentence)\n",
    "        token_word = [self.word_tokenizer(token_sent[i]) for i in range(len(token_sent))]\n",
    "        for no_of_sent in range(len(token_word)):\n",
    "            stem = [(self.stemmer.stem((token_word[no_of_sent][words]))) for words in range(len(token_word[no_of_sent]))]\n",
    "            stem = \" \".join(stem)\n",
    "            print \"tokenized sentence is: \" + str(token_sent[no_of_sent])\n",
    "            print \"stemmer of this sentence is: \" +  str(stem)\n",
    "            print \"tokenized words in stemmer sentence are: \" + str(self.word_tokenizer(stem))\n",
    "            print \"=========================================================================\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porter Stemmer Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Output of porter stemmer algorithm ****\n",
      "===========================================\n",
      "tokenized sentence is: This is first cats.\n",
      "stemmer of this sentence is: Thi is first cat\n",
      "tokenized words in stemmer sentence are: [u'Thi', u'is', u'first', u'cat']\n",
      "=========================================================================\n",
      "tokenized sentence is: Ph.d is very difficult to complete.\n",
      "stemmer of this sentence is: Ph.d is veri difficult to complet\n",
      "tokenized words in stemmer sentence are: [u'Ph.d', u'is', u'veri', u'difficult', u'to', u'complet']\n",
      "=========================================================================\n",
      "tokenized sentence is: My favourite colours are red, orange, green.\n",
      "stemmer of this sentence is: My favourit colour are red orang green\n",
      "tokenized words in stemmer sentence are: [u'My', u'favourit', u'colour', u'are', u'red', u'orang', u'green']\n",
      "=========================================================================\n",
      "tokenized sentence is: Let's try to tokenize the sentences.\n",
      "stemmer of this sentence is: Let 's tri to token the sentenc\n",
      "tokenized words in stemmer sentence are: [u'Let', u\"'s\", u'tri', u'to', u'token', u'the', u'sentenc']\n",
      "=========================================================================\n",
      "tokenized sentence is: how are you?\n",
      "stemmer of this sentence is: how are you\n",
      "tokenized words in stemmer sentence are: [u'how', u'are', u'you']\n",
      "=========================================================================\n",
      "tokenized sentence is: I am doing good\n",
      "stemmer of this sentence is: I am do good\n",
      "tokenized words in stemmer sentence are: [u'I', u'am', u'do', u'good']\n",
      "=========================================================================\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "text_normalizer = text_normalization()\n",
    "# define any sentence\n",
    "sentence = \"This is first cats. Ph.d is very difficult to complete. My favourite colours are red, orange, green. Let's try to tokenize the sentences. how are you? I am doing good\"\n",
    "print \"*** Output of porter stemmer algorithm ****\"\n",
    "print \"===========================================\"\n",
    "\n",
    "text_normalizer.porter_stemmer(sentence)\n",
    "print \"=============================================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Output of Lemmatizing ****\n",
      "===========================================\n",
      "tokenized sentence is: This is first cats.\n",
      "lemma of this sentence is: This is first cat\n",
      "tokenized words in lemmatizer sentence are: [u'This', u'is', u'first', u'cat']\n",
      "=========================================================================\n",
      "tokenized sentence is: Ph.d is very difficult to complete.\n",
      "lemma of this sentence is: Ph.d is very difficult to complete\n",
      "tokenized words in lemmatizer sentence are: ['Ph.d', 'is', 'very', 'difficult', 'to', 'complete']\n",
      "=========================================================================\n",
      "tokenized sentence is: My favourite colours are red, orange, green.\n",
      "lemma of this sentence is: My favourite colour are red orange green\n",
      "tokenized words in lemmatizer sentence are: [u'My', u'favourite', u'colour', u'are', u'red', u'orange', u'green']\n",
      "=========================================================================\n",
      "tokenized sentence is: Let's try to tokenize the sentences.\n",
      "lemma of this sentence is: Let 's try to tokenize the sentence\n",
      "tokenized words in lemmatizer sentence are: [u'Let', u\"'s\", u'try', u'to', u'tokenize', u'the', u'sentence']\n",
      "=========================================================================\n",
      "tokenized sentence is: how are you?\n",
      "lemma of this sentence is: how are you\n",
      "tokenized words in lemmatizer sentence are: ['how', 'are', 'you']\n",
      "=========================================================================\n",
      "tokenized sentence is: I am doing good\n",
      "lemma of this sentence is: I am doing good\n",
      "tokenized words in lemmatizer sentence are: ['I', 'am', 'doing', 'good']\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print \"*** Output of Lemmatizing ****\"\n",
    "print \"===========================================\"\n",
    "\n",
    "text_normalizer.lemmatizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
