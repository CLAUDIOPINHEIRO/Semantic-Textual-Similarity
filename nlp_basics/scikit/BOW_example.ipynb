{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Representation\n",
    "\n",
    "It is the way of transforming the text into the numerical vectors. This transformation is needed because the raw text into many algorithms cannot be feeded and these algos. expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "Most common way to extract numerical features vectors is: \n",
    "\n",
    "    . Tokenization: Way of chopping the sentences into the individual words\n",
    "    . Counting: counting the occurrances of each token in all documents\n",
    "    . Normalizing: normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "    \n",
    "The corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x10 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "# this shows data has two documents/sentences, there are total 10 unique tokens\n",
    "# Therefore sparse matrix is 2 by 10\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of token in each document \n",
      "  [[0 0 0 1 2 1 2 1 1 1]\n",
      " [1 1 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# To check the count of each word in sentence\n",
    "# This does not maintains order\n",
    "print \"count of token in each document \\n \", X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features order \n",
      "  [u'also', u'football', u'games', u'john', u'likes', u'mary', u'movies', u'to', u'too', u'watch']\n"
     ]
    }
   ],
   "source": [
    "# To see the order of the tokens\n",
    "\n",
    "print \"features order \\n \", vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verfying on the unknown text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only two words \"football and games matches\" \n",
    "# Again order of words is not maintained which is drawback of bag of words\n",
    "vectorizer.transform(['Football is one of the most famous games']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the large text corpus, sometimes it is useful to discard the higher frequency terms such as stop words(\"the, a, or, an\") as they might shadow the lower frequency terms(that are most important) in the text. Therefore, we can use another important concept known as **Term frequency- inverse document frequency(TF-IDF) term weighting** Where we can re-weight the count features into floating point values suitable for usage by a classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
